{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date: 03/09/2021\n",
    "\n",
    "Version: 3.0\n",
    "\n",
    "Environment: Python 3.8.3 and Anaconda 6.0.3\n",
    "\n",
    "Operating System: macOS Big Sur (Version 11.5.1)\n",
    "\n",
    "Libraries used:\n",
    "* nltk (Natural Language Toolkit)\n",
    "* re (for regular expressions)\n",
    "* math (for mathemaical function usage)\n",
    "* json (json functions)\n",
    "* nltk.tokenize (for tokenization)\n",
    "* nltk.probability (for processing probabilistic information, e.g FreqDist)\n",
    "* itertools (contains functions for efficient looping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* 1. Introduction\n",
    "* 2. Import libraries\n",
    "* 3. Conversion of PDF to text file\n",
    "* 4. Operations on generated text file\n",
    "* 5. Tokenization\n",
    "* 6. Context-independent stopwords removal\n",
    "* 7. Generate the 200 bigram collocations\n",
    "* 8. Re-tokenize\n",
    "* 9. Stemming (Porter) the unigrams\n",
    "* 10. Removing Context-dependent and Rare Tokens\n",
    "* 11. Generating sorted corpus vocab\n",
    "* 12. CountVectorizer\n",
    "* Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This task wants us to perform text-preprocessing on the designated dataset provided of various cryptocurrency related articles. The dataset provided is in the form of a pdf, which would be required to converted to suitable format so as to preprocess. Preprocessed data is used directly in downtream applications like document summaraization, recommender systems, and learning-to-rand mathods.\n",
    "\n",
    "The main goal of this task is to generate:\n",
    "\n",
    "1. Generate sorted corpus vocabulary `.txt` file.\n",
    "2. For each day, generate the sparse representation (i.e., doc-term matrix) in `.txt` format.\n",
    "\n",
    "\n",
    "Provided resources/datasets:\n",
    "* Sample dataset for test run.\n",
    "* Designated `.pdf` dataset according to student ID.\n",
    "* `stopwords_en.txt` file of english stopwords\n",
    "\n",
    "More details for each task will be given in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:46.123327Z",
     "start_time": "2021-09-03T13:48:45.146895Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import math\n",
    "import json \n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.probability import *\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from __future__ import division\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conversion of PDF to text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform preprocessing we require certain format. As we are given a `.pdf` file, we convert it to `.txt` format using the `pdfminer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:48.558782Z",
     "start_time": "2021-09-03T13:48:46.126058Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer\n",
      "  Using cached pdfminer-20191125.tar.gz (4.2 MB)\n",
      "Collecting pycryptodome\n",
      "  Downloading pycryptodome-3.14.1-cp35-abi3-macosx_10_9_x86_64.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 5.3 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pdfminer\n",
      "  Building wheel for pdfminer (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pdfminer: filename=pdfminer-20191125-py3-none-any.whl size=6140098 sha256=a0add195d1e7f4da741892cda00bfb87f5ba00e09b8125c12d6dd262e7b4fc11\n",
      "  Stored in directory: /Users/abhigambhir/Library/Caches/pip/wheels/d1/aa/48/370f83a970d62355a2a47d2d640094a64eea932c22edae1891\n",
      "Successfully built pdfminer\n",
      "Installing collected packages: pycryptodome, pdfminer\n",
      "Successfully installed pdfminer-20191125 pycryptodome-3.14.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfminer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:55.943247Z",
     "start_time": "2021-09-03T13:48:48.564403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed!\n"
     ]
    }
   ],
   "source": [
    "!pdf2txt.py -o text_31072100.txt '31072100_task2_pdf.pdf'\n",
    "print('Conversion completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Operations on generated text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:55.959564Z",
     "start_time": "2021-09-03T13:48:55.951052Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reading the produced .txt file\n",
    " \n",
    "pdfTxtFile = 'text_31072100.txt'\n",
    "\n",
    "with open(pdfTxtFile) as pdf_txt:\n",
    "    txt_content = pdf_txt.read()\n",
    "#     print(repr(txt_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract only the content and date of articles i.e. discard titles and segregate different articles, we use regular expression and apply `finall()` to get the tuples of `(date, content)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:55.992454Z",
     "start_time": "2021-09-03T13:48:55.963395Z"
    }
   },
   "outputs": [],
   "source": [
    "# re_article = re.compile(\"(?s)(\\[\\d{4}[-|/](?:0?[1-9]|1[0-2])[-|/](?:[0-3]?[0-9])\\]?)(?:.*?\\\\n\\\\n)(.*?)(?=\\[\\d{4}[-|/](?:0?[1-9]|1[0-2])[-|/](?:[0-3]?[0-9])\\]?|$)\")\n",
    "\n",
    "re_article = re.compile(\n",
    "                        \"(?s)\" \n",
    "                        \"(\\[\\d{4}[-|/](?:0?[1-9]|1[0-2])[-|/](?:[0-3]?[0-9])\\]?)\"\n",
    "                        \"(?:.*?\\\\n\\\\n)\"\n",
    "                        \"(.*?)\"\n",
    "                        \"(?=\\[\\d{4}[-|/](?:0?[1-9]|1[0-2])[-|/](?:[0-3]?[0-9])\\]?|$)\"\n",
    "                    )\n",
    "\n",
    "\n",
    "\n",
    "a_list = re_article.findall(txt_content)\n",
    "# a_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:56.010014Z",
     "start_time": "2021-09-03T13:48:55.995628Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "528"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Number of articles we get are `528`. \n",
    "\n",
    "But there is possibility that many articles have same date. In such a case we need to concatenate these articles. This can be done using the dictionary and the following code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:56.029960Z",
     "start_time": "2021-09-03T13:48:56.012363Z"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenating articles with same date\n",
    "\n",
    "articles_dict = {}\n",
    "\n",
    "for element in a_list:\n",
    "        \n",
    "    if element[0] in articles_dict:\n",
    "#         print(element[0])\n",
    "        \n",
    "        articles_dict.update({\n",
    "            element[0]: articles_dict[element[0]] + element[1]\n",
    "        })\n",
    "        \n",
    "#         print('*'*20)\n",
    "#         print(articles_dict[element[0]])\n",
    "    else:\n",
    "        articles_dict[element[0]] = element[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:56.038002Z",
     "start_time": "2021-09-03T13:48:56.034556Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "415"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After concatenating number of articls with unique date came down to `415`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After observing some words in the corpus, we observed some words are getting `\\n` between the word itself. Eg: `bitcoin\\n-unity`, `smart\\n-contracts` especially in urls as these are being continued to next line in the original file. We neeed to handle such scenarios as they can lead us to incorrect or misleading bigrams in future steps. To tackle this we make use of `json` library's `dumps()` to replace them with only `-`. `loads()` bring us back to the state we had before using `dumps()`.\n",
    "\n",
    "Basically what `dumps()` does it is convert our corpus/datatype to `str` so that we can apply str operation on it like `replace()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:56.071338Z",
     "start_time": "2021-09-03T13:48:56.040501Z"
    }
   },
   "outputs": [],
   "source": [
    "urls_correction = json.dumps(articles_dict).replace('\\\\n-','-')\n",
    "\n",
    "articles_dict = json.loads(urls_correction)\n",
    "# articles_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:56.076228Z",
     "start_time": "2021-09-03T13:48:56.073033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "415"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles_dict) # Rechecking that we haven't lost any info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is a process of breaking down a given paragraph of text into a list of sentence or words. When paragraph is broken down into list of sentences, it is called sentence tokenization. Similarly, if the sentences are further broken down into list of words, it is known as Word tokenization.\n",
    "\n",
    "Here we will be performing `Word Tokenization` using the provided Regex in the assignment specification\n",
    "<h4><center>r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"</center></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:56.079812Z",
     "start_time": "2021-09-03T13:48:56.077697Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:56.084626Z",
     "start_time": "2021-09-03T13:48:56.081370Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenizeRawArticleData(date):\n",
    "    \"\"\"\n",
    "     function to tokenize the article paragraph\n",
    "     :param date: date of the article\n",
    "    \"\"\"\n",
    "    raw_article = articles_dict[date].lower()            # lowercasing the whole article text\n",
    "    tokenised_article = tokenizer.tokenize(raw_article)  # applying tokenizer\n",
    "    return (date, tokenised_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:56.122303Z",
     "start_time": "2021-09-03T13:48:56.086271Z"
    }
   },
   "outputs": [],
   "source": [
    "# Applying the function to whole corpus of articles\n",
    "\n",
    "tokenized_corpus = dict(tokenizeRawArticleData(date) for date in articles_dict)\n",
    "# tokenized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T07:27:46.671223Z",
     "start_time": "2021-09-03T07:27:46.666422Z"
    }
   },
   "source": [
    "### Corpus Stats Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:56.127028Z",
     "start_time": "2021-09-03T13:48:56.123816Z"
    }
   },
   "outputs": [],
   "source": [
    "def stats_corpus(tokenized_corpus):\n",
    "    words = list(chain.from_iterable(tokenized_corpus.values()))\n",
    "    vocab = set(words)\n",
    "    lexical_diversity = len(words)/len(vocab)\n",
    "    print (\"Vocabulary size: \",len(vocab),\"\\nTotal number of tokens: \", len(words), \\\n",
    "    \"\\nLexical diversity: \", lexical_diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:56.137512Z",
     "start_time": "2021-09-03T13:48:56.128534Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  11408 \n",
      "Total number of tokens:  93858 \n",
      "Lexical diversity:  8.227384291725105\n"
     ]
    }
   ],
   "source": [
    "stats_corpus(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:56.141758Z",
     "start_time": "2021-09-03T13:48:56.139057Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "415"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_corpus.values())  # Re-assuring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Context-independent stopwords  removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are such words which are very common in occurrence such as 'a','an','the', 'at' etc. We ignore such words during the preprocessing part since they do not give any important information and would just take additional space.\n",
    "\n",
    "There can be two kinds of stopwords:\n",
    "\n",
    "1. Context-independent \n",
    "2. Context-dependent *(later)*\n",
    "\n",
    "We will be using the custom stopwords file `stopwords_en.txt` provided to remove the context-independent stopwords from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:56.146368Z",
     "start_time": "2021-09-03T13:48:56.143106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "571\n"
     ]
    }
   ],
   "source": [
    "# reading the stopwords file into a list where each element will be a stopword\n",
    "\n",
    "with open('stopwords_en.txt') as stop_words_file:\n",
    "    stop_words = stop_words_file.read().splitlines()\n",
    "    \n",
    "print(type(stop_words))\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There are `571` stopwords provided in the the custom file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the list of stopwords to sets using `set()`.\n",
    "\n",
    "**Why convert to sets ?**\n",
    "\n",
    "If you have hashable items, which means both the item order and duplicates are disregarded, Python set is better choice than list as set uses hash. Indeed, set takes constant time to check the membership. Let's convert the stopword list into a stopword set, then search to remove all the stopwords. Please also note that if you try to perform iteration, list is much better than set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:56.158028Z",
     "start_time": "2021-09-03T13:48:56.148333Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words_set = set(stop_words)\n",
    "\n",
    "for date_doc in tokenized_corpus:\n",
    "    tokenized_corpus[date_doc] = [ w for w in tokenized_corpus[date_doc] if w not in stop_words_set ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:56.165344Z",
     "start_time": "2021-09-03T13:48:56.159728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  10923 \n",
      "Total number of tokens:  48748 \n",
      "Lexical diversity:  4.462876499130275\n"
     ]
    }
   ],
   "source": [
    "stats_corpus(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate the 200 bigram collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides unigrams that we have been working on so far,\n",
    "N-grams of texts are also extensively used in various text analysis tasks.\n",
    "They are basically contiguous sequences of `n` words from a given sequence of text.\n",
    "When computing the n-grams you typically move a fixed size window of size n\n",
    "words forward\n",
    "\n",
    "**What are N-grams used for?** They can be used to build n-gram language model that\n",
    "can be further used for speech recognition, spelling correction, entity detection, etc.\n",
    "In terms of text mining tasks, n-grams is used for developing features for \n",
    "classification algorithms, such as SVMs, MaxEnt models, Naive Bayes, etc.\n",
    "The idea is to expand the unigram feature space with n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to concatenate all the tokenized patents using the chain.frome_iterable function. The returned list by the function contains a list of all the words seprated by while space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:56.170807Z",
     "start_time": "2021-09-03T13:48:56.166976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48748"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_tcorpus = list(chain.from_iterable(tokenized_corpus.values()))\n",
    "len(words_tcorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting from a text a list of n-gram can be easily accomplished with function `ngram()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:56.222666Z",
     "start_time": "2021-09-03T13:48:56.172327Z"
    }
   },
   "outputs": [],
   "source": [
    "bi_grams = ngrams(words_tcorpus, n=2)\n",
    "fd_bigram = FreqDist(bi_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocations are expressions of multiple words that commonly co-occur. \n",
    "\n",
    "For example, to extract bigram collocations, we can firstly extract bigrams then get the commonly co-occurring ones by ranking the bigrams by some measures. A commonly used measure is Pointwise Mutual Information (PMI). The following code will find the best 200 bigrams using the PMI scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:56.309895Z",
     "start_time": "2021-09-03T13:48:56.224323Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(words_tcorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:56.493541Z",
     "start_time": "2021-09-03T13:48:56.311712Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a-blockbuster', 'year-for'),\n",
       " ('a-bullish', 'sign-returns'),\n",
       " ('a-kind', 'downloadable'),\n",
       " ('a-peek', 'into-how'),\n",
       " ('abandoned', 'smelter'),\n",
       " ('abolish-cash', 'in-favor'),\n",
       " ('accenture-creates', 'blockchain-editing'),\n",
       " ('accepting-bitcoin', 'in-swi'),\n",
       " ('access-post', 'fork-token'),\n",
       " ('achievements-and', 'aims-for')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_200_bigrams= finder.nbest(bigram_measures.pmi, 200)\n",
    "top_200_bigrams[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:56.498461Z",
     "start_time": "2021-09-03T13:48:56.495477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_200_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Re-tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have 200 bigrams from corpus which we do not want split into two individual words. So we do retokenize using `MWEtokenizer`\n",
    "\n",
    "> A ``MWETokenizer`` takes a string which has already been divided into tokens and\n",
    "retokenizes it, merging multi-word expressions into single tokens, using a lexicon\n",
    "of MWEs: [Source](https://www.nltk.org/_modules/nltk/tokenize/mwe.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:56.517435Z",
     "start_time": "2021-09-03T13:48:56.499976Z"
    }
   },
   "outputs": [],
   "source": [
    "mwe_tokenizer = MWETokenizer(top_200_bigrams)\n",
    "\n",
    "collocated_articles = dict((date, mwe_tokenizer.tokenize(article)) for date,article in tokenized_corpus.items())\n",
    "\n",
    "all_collocated_words = list(chain.from_iterable(collocated_articles.values()))\n",
    "collocated_vocab = list(set(all_collocated_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:56.521575Z",
     "start_time": "2021-09-03T13:48:56.519205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48555\n",
      "10730\n"
     ]
    }
   ],
   "source": [
    "print(len(all_collocated_words))\n",
    "print(len(collocated_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * `48555` words in our `collocated_articles` corpus\n",
    "* `10730` unique words in our `collocated_articles` corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Stemming (Porter) the unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming means mapping a group of words to the same stem by removing prefixes or suffixes without giving any value to the “grammatical meaning” of the stem formed after the process.\n",
    "\n",
    "e.g.\n",
    "\n",
    "computation --> comput\n",
    "\n",
    "computer --> comput \n",
    "\n",
    "hobbies --> hobbi\n",
    "\n",
    "We can see that stemming tries to bring the word back to their base word but the base word may or may not have correct grammatical meanings.\n",
    "\n",
    "We will be using `PorterStemmer` (as per specification) available in NLTK package. Porter stemmer is the oldest algorithm present and was the most popular to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Important**\n",
    "\n",
    "* Since we want our bigrams in the corpus to remain intact, be don't apply stemming on them. Stemming is being applied only to unigrams by the following code. To not stem bigrams we check if `_` is a character in the word we keep as it is.\n",
    "\n",
    "```python\n",
    "[ ... stemmer.stem(collocated_articles[date_article][w]) if '_' not in collocated_articles[date_article][w] else collocated_articles[date_article][w] ... ] \n",
    "```\n",
    "\n",
    "* We need to discard words with length smaller than 3, so keep only the words having `length >= 3` and this is applied to whole corpus (unigrams or bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:57.438155Z",
     "start_time": "2021-09-03T13:48:56.523027Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "uni_bigram_articles = {}\n",
    "\n",
    "for date_article in collocated_articles:\n",
    "    uni_bigram_articles[date_article] = [ stemmer.stem(collocated_articles[date_article][w]) if '_' not in collocated_articles[date_article][w] else collocated_articles[date_article][w] for w in range(len(collocated_articles[date_article])) ] \n",
    "    \n",
    "    # Removing words with < 3 length\n",
    "    uni_bigram_articles[date_article] = [ w for w in uni_bigram_articles[date_article] if len(w) >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:57.916803Z",
     "start_time": "2021-09-03T13:48:57.440200Z"
    }
   },
   "outputs": [],
   "source": [
    "# uni_bigram_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:57.925449Z",
     "start_time": "2021-09-03T13:48:57.918481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  8050 \n",
      "Total number of tokens:  48161 \n",
      "Lexical diversity:  5.9827329192546586\n"
     ]
    }
   ],
   "source": [
    "stats_corpus(uni_bigram_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Removing Context-dependent and Rare Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For removing context-dependent and rare tokens we need to follow the following rules:\n",
    "* For context-dependent stopwords, you must set the threshold to more than `ceil(Number_of_days / 2)`.\n",
    "* Rare tokens (with the threshold set to less than `10 days`) must be removed from the vocab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T08:35:44.501238Z",
     "start_time": "2021-09-03T08:35:44.495469Z"
    }
   },
   "source": [
    "From the rules, it is clear that we need to take into account the **document frequency** for both scenarios.\n",
    "\n",
    "> Document frequency is the number of documents containing a particular term (regardless of how many times it occurs in the same document)\n",
    "\n",
    "Our approach will be to collect both the context dependent and rare tokens in the single list `final_remove_tokens` and remove them from corpus in the similar way we remove stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:57.957110Z",
     "start_time": "2021-09-03T13:48:57.927158Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'http': 381, 'bitcoin': 327, 'news': 204, 'cryptocurr': 180, 'www': 172, 'read': 154, 'blockchain': 147, 'currenc': 140, 'market': 136, 'time': 130, ...})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words = list(chain.from_iterable([set(value) for value in uni_bigram_articles.values()]))\n",
    "df_ub_grams = FreqDist(df_words)\n",
    "df_ub_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: It is highly possible that bigrams occur `lesser than 10` and will be considered as rare tokens. This is not what we want so we apply rare tokens condition only on unigrams and not on bigrams using the following snippet inside below code:\n",
    "\n",
    "```python\n",
    "...\n",
    "if token_docfreq < 10  and '_' not in token:\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:57.967422Z",
     "start_time": "2021-09-03T13:48:57.959183Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7097"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_dep_stopwords = []\n",
    "rare_tokens = []\n",
    "\n",
    "number_of_days = len(uni_bigram_articles)\n",
    "\n",
    "for token, token_docfreq in df_ub_grams.items():\n",
    "    \n",
    "    # if token's doc-freq is greater then the condition append it to list\n",
    "    if token_docfreq > math.ceil(number_of_days/2):\n",
    "        context_dep_stopwords.append(token)\n",
    "        \n",
    "    # if token's doc-freq is smaller then 10 and token is not bigram, append it to list\n",
    "    if token_docfreq < 10  and '_' not in token:\n",
    "        rare_tokens.append(token)\n",
    "\n",
    "# set because it uses hash and can be removed faster from corpus    \n",
    "final_remove_tokens = set(context_dep_stopwords + rare_tokens)   \n",
    "len(final_remove_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T08:51:40.335709Z",
     "start_time": "2021-09-03T08:51:40.328937Z"
    }
   },
   "source": [
    "> There are `7097` tokens (includes both context-dependent stopwords and rare tokens) we need to remove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:57.975613Z",
     "start_time": "2021-09-03T13:48:57.968943Z"
    }
   },
   "outputs": [],
   "source": [
    "for article_date in uni_bigram_articles:\n",
    "    uni_bigram_articles[article_date] = [ w for w in uni_bigram_articles[article_date] if w not in final_remove_tokens ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:57.982024Z",
     "start_time": "2021-09-03T13:48:57.977588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  953 \n",
      "Total number of tokens:  29773 \n",
      "Lexical diversity:  31.24134312696747\n"
     ]
    }
   ],
   "source": [
    "stats_corpus(uni_bigram_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Generating sorted corpus vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task has to generate a file with specified format containing words/tokens in corpus with it's respective term frequency (TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:58.000863Z",
     "start_time": "2021-09-03T13:48:57.983526Z"
    }
   },
   "outputs": [],
   "source": [
    "uni_bigram_words = list(chain.from_iterable([value for value in uni_bigram_articles.values()]))\n",
    "tokens_freq_dict = FreqDist(uni_bigram_words)\n",
    "# len(set(uni_bigram_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:58.005538Z",
     "start_time": "2021-09-03T13:48:58.002774Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'cryptocurr': 594, 'blockchain': 381, 'market': 375, 'exchang': 334, 'currenc': 323, 'bank': 310, 'price': 308, 'trade': 276, 'news': 266, 'digit': 244, ...})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:58.009037Z",
     "start_time": "2021-09-03T13:48:58.006808Z"
    }
   },
   "outputs": [],
   "source": [
    "final_vocab_dict = dict(tokens_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:58.013713Z",
     "start_time": "2021-09-03T13:48:58.010837Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "953"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:58.019827Z",
     "start_time": "2021-09-03T13:48:58.015174Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_output_file = open('31072100_vocab.txt', \"w\")\n",
    "sorted_vocab = sorted(final_vocab_dict.items())\n",
    "        \n",
    "for word, freq in sorted_vocab:  \n",
    "    vocab_output_file.write(str(word) + ':' + str(freq) + '\\n')\n",
    "\n",
    "vocab_output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:58.024906Z",
     "start_time": "2021-09-03T13:48:58.021076Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", vocabulary=set(uni_bigram_words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:58.048510Z",
     "start_time": "2021-09-03T13:48:58.026529Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(415, 953)\n"
     ]
    }
   ],
   "source": [
    "data_features = vectorizer.fit_transform([' '.join(value) for value in uni_bigram_articles.values()])\n",
    "print(data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:58.054410Z",
     "start_time": "2021-09-03T13:48:58.050030Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(uni_bigram_words)-set(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:58.060045Z",
     "start_time": "2021-09-03T13:48:58.056101Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = sorted(set(uni_bigram_words))\n",
    "vocab2 = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:58.063539Z",
     "start_time": "2021-09-03T13:48:58.061470Z"
    }
   },
   "outputs": [],
   "source": [
    "dates_articles = list(uni_bigram_articles.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:58.067383Z",
     "start_time": "2021-09-03T13:48:58.064929Z"
    }
   },
   "outputs": [],
   "source": [
    "count_vec_file = open('31072100_countVec.txt', \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:58.271305Z",
     "start_time": "2021-09-03T13:48:58.068855Z"
    }
   },
   "outputs": [],
   "source": [
    "flag = False\n",
    "for row in range(data_features.shape[0]+1):\n",
    "    if row == data_features.shape[0]:\n",
    "        count_vec_file.write(output_str.rstrip(',') + '\\n')\n",
    "    else:\n",
    "        article_date = dates_articles[row]\n",
    "        if flag == True:\n",
    "            count_vec_file.write(output_str.rstrip(',') + '\\n')\n",
    "        output_str = str(article_date)+','\n",
    "        for idx, count in enumerate(data_features.toarray()[row], 1):\n",
    "            if count > 0:\n",
    "                output_str += str(idx) + ':' + str(count) + ','\n",
    "                flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:48:58.275675Z",
     "start_time": "2021-09-03T13:48:58.273195Z"
    }
   },
   "outputs": [],
   "source": [
    "count_vec_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T12:56:37.507533Z",
     "start_time": "2021-09-03T12:56:37.504931Z"
    }
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T13:35:52.371674Z",
     "start_time": "2021-09-03T13:35:52.366760Z"
    }
   },
   "source": [
    "The task was performed and two required files of vocab and countvector was created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* Tut 5 Solutions, FIT5196, Monash University, Link Confedential\n",
    "* Tut 4 Solutions, FIT5196, Monash University, Link Confedential"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
